{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torchvision\n",
    "!pip install tqdm\n",
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "\n",
    "DEVICE = \"cpu\"          # CPU-only mode (as requested)\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "# ========== Federated Learning Parameters ============\n",
    "NUM_CLIENTS = 10\n",
    "ROUNDS = 5\n",
    "LOCAL_EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# ========== Prompt Tuning Settings ===================\n",
    "PROMPT_LEN = 16\n",
    "EMBED_DIM = 512           # CLIP ViT-B/16 text embedding dim\n",
    "RANK = 8                  # As used in the paper\n",
    "LR_LOCAL = 1e-3\n",
    "LR_GLOBAL = 1e-3\n",
    "\n",
    "# ========== Differential Privacy Parameters ==========\n",
    "EPSILONS = [0.4, 0.2, 0.1, 0.05, 0.01]   # Paper settings\n",
    "DELTA = 1e-5                              # Paper setting\n",
    "CLIP_THRESHOLD = 10                       # Paper setting\n",
    "\n",
    "print(\"Config loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "hf_clip.eval()\n",
    "for p in hf_clip.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Loaded HuggingFace CLIP ViT-B/16 (frozen).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"train\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"test\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "labels = np.array(trainset._labels)\n",
    "print(\"Train samples:\", len(trainset))\n",
    "print(\"Test samples:\", len(testset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_by_class = {i: np.where(labels == i)[0] for i in range(102)}\n",
    "\n",
    "classes_per_client = 102 // NUM_CLIENTS\n",
    "client_indices = []\n",
    "\n",
    "start = 0\n",
    "for cid in range(NUM_CLIENTS):\n",
    "    selected_classes = list(range(start, start + classes_per_client))\n",
    "    start += classes_per_client\n",
    "\n",
    "    indices = []\n",
    "    for c in selected_classes:\n",
    "        indices.extend(idx_by_class[c])\n",
    "    \n",
    "    client_indices.append(indices)\n",
    "\n",
    "print(\"Example: Client 0 samples =\", len(client_indices[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_prompt = nn.Parameter(torch.randn(PROMPT_LEN, EMBED_DIM))\n",
    "        self.local_prompt  = nn.Parameter(torch.randn(PROMPT_LEN, EMBED_DIM))\n",
    "\n",
    "    def full_prompt(self):\n",
    "        return self.global_prompt + self.local_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b807126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_factorization(mat, rank=RANK):\n",
    "    Q = torch.randn(mat.size(1), rank)\n",
    "    Q = torch.linalg.qr(Q).Q\n",
    "\n",
    "    # 1-step power iteration\n",
    "    Z = mat.T @ (mat @ Q)\n",
    "    Q = torch.linalg.qr(Z).Q\n",
    "\n",
    "    u = mat @ Q\n",
    "    v = Q.T\n",
    "    approx = u @ v\n",
    "    residual = mat - approx\n",
    "    return u, v, residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigma(sensitivity, epsilon, delta):\n",
    "    \"\"\"\n",
    "    Gaussian mechanism noise:\n",
    "    σ >= sqrt(2 * log(1.25/delta)) * sensitivity / epsilon\n",
    "    \"\"\"\n",
    "    return math.sqrt(2 * math.log(1.25/delta)) * sensitivity / epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, cid, indices, global_model):\n",
    "        self.cid = cid\n",
    "        self.indices = indices\n",
    "        self.model = copy.deepcopy(global_model)\n",
    "\n",
    "    def get_loader(self):\n",
    "        ds = Subset(trainset, self.indices)\n",
    "        return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    def local_train(self, sigma_l):\n",
    "        loader = self.get_loader()\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=LR_LOCAL)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for _ in range(LOCAL_EPOCHS):\n",
    "            for img, label in loader:\n",
    "                batch = processor(images=img, return_tensors=\"pt\")\n",
    "                pixel_values = batch[\"pixel_values\"]\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # low-rank factorization\n",
    "                u, v, r = low_rank_factorization(self.model.local_prompt)\n",
    "                p_local = u @ v + r\n",
    "\n",
    "                # encode text\n",
    "                text_features = hf_clip.text_model(\n",
    "                    p_local.unsqueeze(0)\n",
    "                ).last_hidden_state.mean(1)\n",
    "\n",
    "                # encode images\n",
    "                img_features = hf_clip.vision_model(pixel_values).pooler_output\n",
    "\n",
    "                logits = (text_features @ img_features.T) / 0.07\n",
    "                loss = F.cross_entropy(logits, label)\n",
    "                loss.backward()\n",
    "\n",
    "                # TRUE LDP clipping + noise\n",
    "                for name, p in self.model.named_parameters():\n",
    "                    if \"local_prompt\" in name and p.grad is not None:\n",
    "                        p.grad = torch.clamp(p.grad, -CLIP_THRESHOLD, CLIP_THRESHOLD)\n",
    "                        p.grad += torch.randn_like(p.grad) * sigma_l\n",
    "\n",
    "                opt.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        return np.mean(losses)\n",
    "\n",
    "    def get_global_grad(self, sigma_g):\n",
    "        # Global DP: return Gaussian noise gradient\n",
    "        return torch.randn_like(self.model.global_prompt) * sigma_g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self):\n",
    "        self.global_model = PromptLearner()\n",
    "        self.clients = [\n",
    "            Client(cid, client_indices[cid], self.global_model)\n",
    "            for cid in range(NUM_CLIENTS)\n",
    "        ]\n",
    "\n",
    "    def aggregate(self, grads, sigma_g):\n",
    "        mean_grad = torch.stack(grads).mean(dim=0)\n",
    "        # add global DP noise\n",
    "        noisy_grad = mean_grad + torch.randn_like(mean_grad) * sigma_g\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.global_model.global_prompt -= LR_GLOBAL * noisy_grad\n",
    "\n",
    "    def run(self, epsilon):\n",
    "        print(f\"\\n=== Training with ε = {epsilon} ===\")\n",
    "\n",
    "        # compute sensitivities\n",
    "        SL = CLIP_THRESHOLD / BATCH_SIZE\n",
    "        SG = CLIP_THRESHOLD / (NUM_CLIENTS * BATCH_SIZE)\n",
    "\n",
    "        sigma_l = compute_sigma(SL, epsilon, DELTA)\n",
    "        sigma_g = compute_sigma(SG, epsilon, DELTA)\n",
    "\n",
    "        print(f\"sigma_L={sigma_l:.4f}, sigma_G={sigma_g:.4f}\")\n",
    "\n",
    "        for r in range(ROUNDS):\n",
    "            print(f\"\\n--- Round {r+1}/{ROUNDS} ---\")\n",
    "\n",
    "            grads = []\n",
    "            losses = []\n",
    "\n",
    "            for client in tqdm(self.clients):\n",
    "                client.model.global_prompt.data = self.global_model.global_prompt.data.clone()\n",
    "\n",
    "                losses.append(client.local_train(sigma_l))\n",
    "                grads.append(client.get_global_grad(sigma_g))\n",
    "\n",
    "            self.aggregate(grads, sigma_g)\n",
    "            print(\"Avg local loss:\", np.mean(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b01c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server()\n",
    "\n",
    "for eps in EPSILONS:\n",
    "    server.run(eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c53363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_client(client):\n",
    "    client.model.eval()\n",
    "    loader = DataLoader(testset, batch_size=32)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    prompt = client.model.full_prompt()\n",
    "    text_features = hf_clip.text_model(\n",
    "        prompt.unsqueeze(0)\n",
    "    ).last_hidden_state.mean(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in loader:\n",
    "            pixel_values = processor(images=img, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "            img_features = hf_clip.vision_model(pixel_values).pooler_output\n",
    "\n",
    "            logits = (text_features @ img_features.T) / 0.07\n",
    "            pred = logits.argmax(dim=0)\n",
    "\n",
    "            correct += (pred == label).sum().item()\n",
    "            total += len(label)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "for cid, client in enumerate(server.clients):\n",
    "    print(f\"Client {cid} Accuracy = {evaluate_client(client):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
