{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f53e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implements:\n",
    "\n",
    "# Federated Prompt Learning (10 clients)\n",
    "\n",
    "# OpenAI CLIP (ViT-B/32, frozen encoder)\n",
    "\n",
    "# Local & Global prompts\n",
    "\n",
    "# Low-rank factorization + residual\n",
    "\n",
    "# Simulated DP noise (fast!)\n",
    "\n",
    "# Personalization (local) vs Generalization (neighbor) evaluation\n",
    "\n",
    "## --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e164620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/bg/wsgmhpr97ms7nhlct5rkgzn40000gn/T/pip-req-build-y8vdpjpc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/bg/wsgmhpr97ms7nhlct5rkgzn40000gn/T/pip-req-build-y8vdpjpc\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy (from clip==1.0)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.13/site-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.13/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (from clip==1.0) (2.9.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.13/site-packages (from clip==1.0) (0.24.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.13/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch->clip==1.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch->clip==1.0) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch->clip==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch->clip==1.0) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torchvision->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from torchvision->clip==1.0) (11.1.0)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Building wheels for collected packages: clip\n",
      "\u001b[33m  DEPRECATION: Building 'clip' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'clip'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369491 sha256=34729dcb71e9f1e93466ac4496ef84475470ddabae7ae72932d4bd316beaa107\n",
      "  Stored in directory: /private/var/folders/bg/wsgmhpr97ms7nhlct5rkgzn40000gn/T/pip-ephem-wheel-cache-m_vp18r7/wheels/cb/a8/74/5f32d6cf0407457f0f62737b6da5c14eb86b9cac476fdf630d\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, clip\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [clip]\n",
      "\u001b[1A\u001b[2KSuccessfully installed clip-1.0 ftfy-6.3.1\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.13/site-packages (0.24.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: torch==2.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (2.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.0->torchvision) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.0->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch==2.9.0->torchvision) (3.0.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install torchvision\n",
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import clip\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "\n",
    "DEVICE = \"cpu\"   # CPU ONLY as requested\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "# Federated setup\n",
    "NUM_CLIENTS = 10\n",
    "ROUNDS = 5\n",
    "LOCAL_EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Prompt settings\n",
    "PROMPT_LEN = 16\n",
    "EMBED_DIM = 512   # CLIP ViT-B/32 text dim\n",
    "\n",
    "# DP Simulation noise levels (fast)\n",
    "SIGMA_L = 0.3    # \"local\" DP noise\n",
    "SIGMA_G = 0.1    # \"global\" DP noise\n",
    "\n",
    "# Low-rank factorization\n",
    "RANK = 4\n",
    "LR_LOCAL = 1e-3\n",
    "LR_GLOBAL = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Loaded CLIP on\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = preprocess\n",
    "\n",
    "dataset = torchvision.datasets.Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"train\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.Flowers102(\n",
    "    root=\"./data\",\n",
    "    split=\"test\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "num_samples = len(dataset)\n",
    "print(\"Training samples:\", num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(dataset._labels)\n",
    "\n",
    "# 102 classes → divide into 10 clients\n",
    "classes_per_client = 102 // NUM_CLIENTS\n",
    "client_classes = {}\n",
    "\n",
    "idx_by_class = {i: np.where(labels == i)[0] for i in range(102)}\n",
    "\n",
    "client_indices = []\n",
    "\n",
    "start = 0\n",
    "for cid in range(NUM_CLIENTS):\n",
    "    cls_range = list(range(start, start + classes_per_client))\n",
    "    start += classes_per_client\n",
    "    \n",
    "    indices = []\n",
    "    for c in cls_range:\n",
    "        indices.extend(idx_by_class[c])\n",
    "    \n",
    "    client_indices.append(indices)\n",
    "\n",
    "print(\"Client 0 has\", len(client_indices[0]), \"samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1adfcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_prompt = nn.Parameter(\n",
    "            torch.randn(PROMPT_LEN, EMBED_DIM)\n",
    "        )\n",
    "        self.local_prompt = nn.Parameter(\n",
    "            torch.randn(PROMPT_LEN, EMBED_DIM)\n",
    "        )\n",
    "    \n",
    "    def full_prompt(self):\n",
    "        return self.global_prompt + self.local_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_factorization(mat, rank=RANK):\n",
    "    \"\"\"\n",
    "    mat: (PROMPT_LEN x EMBED_DIM)\n",
    "    returns: u (PROMPT_LEN x rank), v (rank x EMBED_DIM), residual\n",
    "    \"\"\"\n",
    "    # random projection for power iteration\n",
    "    Q = torch.randn(mat.shape[1], rank)\n",
    "    Q = torch.linalg.qr(Q).Q\n",
    "    \n",
    "    # one power iteration (fast)\n",
    "    Z = mat.T @ (mat @ Q)\n",
    "    Q = torch.linalg.qr(Z).Q\n",
    "    \n",
    "    u = mat @ Q\n",
    "    v = Q.T\n",
    "    approx = u @ v\n",
    "    residual = mat - approx\n",
    "    return u, v, residual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68532c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, cid, indices, global_model):\n",
    "        self.cid = cid\n",
    "        self.indices = indices\n",
    "        self.model = copy.deepcopy(global_model)\n",
    "    \n",
    "    def get_loader(self):\n",
    "        ds = Subset(dataset, self.indices)\n",
    "        return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    def local_train(self):\n",
    "        loader = self.get_loader()\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=LR_LOCAL)\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for _ in range(LOCAL_EPOCHS):\n",
    "            for img, label in loader:\n",
    "                img = img.to(DEVICE)\n",
    "                \n",
    "                opt.zero_grad()\n",
    "\n",
    "                # Factorize local prompt into low rank + residual\n",
    "                mat = self.model.local_prompt\n",
    "                u, v, residual = low_rank_factorization(mat)\n",
    "\n",
    "                # Reconstructed prompt\n",
    "                p_local = u @ v + residual\n",
    "                p_global = self.model.global_prompt\n",
    "\n",
    "                # Encode prompt via CLIP text encoder\n",
    "                text_embed = clip_model.encode_text(p_local)\n",
    "                img_embed = clip_model.encode_image(img)\n",
    "\n",
    "                logits = (text_embed @ img_embed.T) / 0.07\n",
    "                loss = F.cross_entropy(logits, label.to(DEVICE))\n",
    "                loss.backward()\n",
    "\n",
    "                # Simulated LDP noise\n",
    "                with torch.no_grad():\n",
    "                    for name, p in self.model.named_parameters():\n",
    "                        if \"local_prompt\" in name:\n",
    "                            if p.grad is not None:\n",
    "                                p.grad += torch.randn_like(p.grad) * SIGMA_L\n",
    "\n",
    "                opt.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        return np.mean(losses)\n",
    "    \n",
    "    def get_global_gradient(self):\n",
    "        # Simulated GDP global gradient (noise-only)\n",
    "        grad = torch.randn_like(self.model.global_prompt) * SIGMA_G\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd49b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self):\n",
    "        self.global_model = PromptLearner().to(DEVICE)\n",
    "        self.clients = [\n",
    "            Client(cid, client_indices[cid], self.global_model)\n",
    "            for cid in range(NUM_CLIENTS)\n",
    "        ]\n",
    "    \n",
    "    def aggregate(self, grads):\n",
    "        mean_grad = torch.stack(grads).mean(dim=0)\n",
    "        with torch.no_grad():\n",
    "            self.global_model.global_prompt -= LR_GLOBAL * mean_grad\n",
    "    \n",
    "    def run(self):\n",
    "        for r in range(ROUNDS):\n",
    "            print(f\"\\n=== Round {r+1}/{ROUNDS} ===\")\n",
    "            grads = []\n",
    "            losses = []\n",
    "\n",
    "            for client in tqdm(self.clients):\n",
    "                # broadcast global prompt\n",
    "                client.model.global_prompt.data = self.global_model.global_prompt.data.clone()\n",
    "\n",
    "                loss = client.local_train()\n",
    "                losses.append(loss)\n",
    "\n",
    "                grads.append(client.get_global_gradient())\n",
    "            \n",
    "            self.aggregate(grads)\n",
    "            print(f\"Avg Local Loss: {np.mean(losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server()\n",
    "server.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df295a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_client(client, mode=\"local\"):\n",
    "    client.model.eval()\n",
    "    test_loader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prompt = client.model.full_prompt()\n",
    "\n",
    "        text_embed = clip_model.encode_text(prompt)\n",
    "\n",
    "        for img, label in test_loader:\n",
    "            img = img.to(DEVICE)\n",
    "            img_emb = clip_model.encode_image(img)\n",
    "\n",
    "            logits = (text_embed @ img_emb.T) / 0.07\n",
    "            pred = logits.argmax(dim=0)\n",
    "\n",
    "            # local classes only\n",
    "            if mode == \"local\":\n",
    "                mask = torch.tensor(\n",
    "                    [label in client.indices for label in range(len(testset))]\n",
    "                )\n",
    "            \n",
    "            correct += (pred.cpu() == label).sum().item()\n",
    "            total += len(label)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# evaluate all clients\n",
    "for cid, client in enumerate(server.clients):\n",
    "    print(f\"Client {cid} | Personalization Acc = {evaluate_client(client):.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
